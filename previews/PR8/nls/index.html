<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>nls · GALAHAD.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="GALAHAD.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">GALAHAD.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../arc/">arc</a></li><li><a class="tocitem" href="../bgo/">bgo</a></li><li><a class="tocitem" href="../blls/">blls</a></li><li><a class="tocitem" href="../bqpb/">bqpb</a></li><li><a class="tocitem" href="../bqp/">bqp</a></li><li><a class="tocitem" href="../bsc/">bsc</a></li><li><a class="tocitem" href="../ccqp/">ccqp</a></li><li><a class="tocitem" href="../clls/">clls</a></li><li><a class="tocitem" href="../convert/">convert</a></li><li><a class="tocitem" href="../cqp/">cqp</a></li><li><a class="tocitem" href="../cro/">cro</a></li><li><a class="tocitem" href="../dgo/">dgo</a></li><li><a class="tocitem" href="../dps/">dps</a></li><li><a class="tocitem" href="../eqp/">eqp</a></li><li><a class="tocitem" href="../fdc/">fdc</a></li><li><a class="tocitem" href="../fit/">fit</a></li><li><a class="tocitem" href="../gls/">gls</a></li><li><a class="tocitem" href="../gltr/">gltr</a></li><li><a class="tocitem" href="../hash/">hash</a></li><li><a class="tocitem" href="../icfs/">icfs</a></li><li><a class="tocitem" href="../ir/">ir</a></li><li><a class="tocitem" href="../lhs/">lhs</a></li><li><a class="tocitem" href="../lms/">lms</a></li><li><a class="tocitem" href="../lpa/">lpa</a></li><li><a class="tocitem" href="../lpb/">lpb</a></li><li><a class="tocitem" href="../lsqp/">lsqp</a></li><li><a class="tocitem" href="../lsrt/">lsrt</a></li><li><a class="tocitem" href="../lstr/">lstr</a></li><li class="is-active"><a class="tocitem" href>nls</a><ul class="internal"><li><a class="tocitem" href="#Purpose"><span>Purpose</span></a></li><li><a class="tocitem" href="#Authors"><span>Authors</span></a></li><li><a class="tocitem" href="#Originally-released"><span>Originally released</span></a></li><li><a class="tocitem" href="#Terminology"><span>Terminology</span></a></li><li><a class="tocitem" href="#Method"><span>Method</span></a></li><li><a class="tocitem" href="#Reference"><span>Reference</span></a></li><li><a class="tocitem" href="#Call-order"><span>Call order</span></a></li><li><a class="tocitem" href="#Unsymmetric-matrix-storage-formats"><span>Unsymmetric matrix storage formats</span></a></li><li><a class="tocitem" href="#Symmetric-matrix-storage-formats"><span>Symmetric matrix storage formats</span></a></li></ul></li><li><a class="tocitem" href="../presolve/">presolve</a></li><li><a class="tocitem" href="../psls/">psls</a></li><li><a class="tocitem" href="../qpb/">qpb</a></li><li><a class="tocitem" href="../roots/">roots</a></li><li><a class="tocitem" href="../rpd/">rpd</a></li><li><a class="tocitem" href="../rqs/">rqs</a></li><li><a class="tocitem" href="../sbls/">sbls</a></li><li><a class="tocitem" href="../scu/">scu</a></li><li><a class="tocitem" href="../sec/">sec</a></li><li><a class="tocitem" href="../sha/">sha</a></li><li><a class="tocitem" href="../sils/">sils</a></li><li><a class="tocitem" href="../slls/">slls</a></li><li><a class="tocitem" href="../sls/">sls</a></li><li><a class="tocitem" href="../trb/">trb</a></li><li><a class="tocitem" href="../trs/">trs</a></li><li><a class="tocitem" href="../tru/">tru</a></li><li><a class="tocitem" href="../ugo/">ugo</a></li><li><a class="tocitem" href="../uls/">uls</a></li><li><a class="tocitem" href="../wcp/">wcp</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>nls</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>nls</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/amontoison/GALAHAD.jl/blob/main/docs/src/nls.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h1><h2 id="Purpose"><a class="docs-heading-anchor" href="#Purpose">Purpose</a><a id="Purpose-1"></a><a class="docs-heading-anchor-permalink" href="#Purpose" title="Permalink"></a></h2><p>This package uses a <strong>regularization method to find a (local) unconstrained minimizer of a differentiable weighted sum-of-squares objective function <span>$\mathbf{f(x) :=  \frac{1}{2} \sum_{i=1}^m w_i^{} c_i^2(x) \equiv \frac{1}{2} \|c(x)\|^2_W}$</span> \n f(x):= 1/2 sum<em>{i=1}^m w</em>i c<em>i^2(x) = 1/2 ||c(x)||^2</em>W \n of many variables <span>$\mathbf{x}$</span> involving positive weights <span>$\mathbf{w_i}$</span>, <span>$\mathbf{i=1,\ldots,m}$</span>.</strong> The method offers the choice of direct and iterative solution of the key regularization subproblems, and is most suitable for large problems. First derivatives of the &lt;i&gt;residual function&lt;/i&gt; <span>$c(x)$</span> are required, and if second derivatives of the <span>$c_i(x)$</span> can be calculated, they may be exploited–-if suitable products of the first or second derivatives with a vector may be found but not the derivatives themselves, that can also be used to advantage.</p><h2 id="Authors"><a class="docs-heading-anchor" href="#Authors">Authors</a><a id="Authors-1"></a><a class="docs-heading-anchor-permalink" href="#Authors" title="Permalink"></a></h2><p>N. I. M. Gould, STFC-Rutherford Appleton Laboratory, England.</p><p>C interface, additionally J. Fowkes, STFC-Rutherford Appleton Laboratory.</p><p>Julia interface, additionally A. Montoison and D. Orban, Polytechnique Montréal.</p><h2 id="Originally-released"><a class="docs-heading-anchor" href="#Originally-released">Originally released</a><a id="Originally-released-1"></a><a class="docs-heading-anchor-permalink" href="#Originally-released" title="Permalink"></a></h2><p>October 2016, C interface August 2021.</p><h2 id="Terminology"><a class="docs-heading-anchor" href="#Terminology">Terminology</a><a id="Terminology-1"></a><a class="docs-heading-anchor-permalink" href="#Terminology" title="Permalink"></a></h2><p>The gradient <span>$\nabla_x f(x)$</span> of a function <span>$f(x)$</span> is the vector whose <span>$i$</span>-th component is <span>$\partial f(x)/\partial x_i$</span>. The Hessian <span>$\nabla_{xx} f(x)$</span> of <span>$f(x)$</span> is the symmetric matrix whose <span>$i,j$</span>-th entry is <span>$\partial^2 f(x)/\partial x_i \partial x_j$</span>. The Hessian is sparse if a significant and useful proportion of the entries are universally zero.</p><p>The algorithm used by the package is iterative. From the current best estimate of the minimizer <span>$x_k$</span>, a trial improved point <span>$x_k + s_k$</span> is sought. The correction <span>$s_k$</span> is chosen to improve a model <span>$m_k(s)$</span> of %the stabilised objective function <span>$f_{\rho,p}(x_k+s)$</span> built around the objective function <span>$f(x_k+s)$</span> built around <span>$x_k$</span>. The model is the sum of two basic components, a suitable approximation <span>$t_k(s)$</span> of <span>$f(x_k+s)$</span>, %another approximation of <span>$(\rho/r) \|x_k+s\|_r^r$</span> (if <span>$\rho &gt; 0$</span>), and a regularization term <span>$(\sigma_k/p) \|s\|_{S_k}^p$</span> involving a weight <span>$\sigma_k$</span>, power <span>$p$</span> and a norm <span>$\|s\|_{S_k} := \sqrt{s^T S_k s}$</span> for a given positive definite scaling matrix <span>$S_k$</span> that is included to prevent large corrections. The weight<span>$\sigma_k$</span> is adjusted as the algorithm progresses toensure convergence.</p><p>The model <span>$t_k(s)$</span> is a truncated Taylor-series approximation, and this relies on being able to compute or estimate derivatives of <span>$c(x)$</span>. Various models are provided, and each has different derivative requirements. We denote the <span>$m$</span> by <span>$n$</span> &lt;i&gt;residual Jacobian&lt;/i&gt; <span>$J(x) \equiv \nabla_x c(x)$</span> as the matrixwhose <span>$i,j$</span>-th component <span>$J(x)_{i,j} := \partial c_i(x) / \partial x_j \;\; \mbox{for $i=1,\ldots,m$</span> and <span>$j=1,\ldots,n$</span>.}$ \n J(x)<em>{i,j} := partial c</em>i(x) / \partial x<em>j \n \manonly for i=1,...,m and j=1,...,n.For a given <span>$m$</span>-vector <span>$y$</span>, the &lt;i&gt;weighted-residual Hessian&lt;/i&gt; is the sum H(x,y) := \sum</em>{\ell=1}^m y<em>\ell H</em>\ell(x), \;\; \mbox{where}\;\; H<em>\ell(x)</em>{i,j} := \partial^2 c<em>\ell(x) / \partial x</em>i \partial x<em>j \;\; \mbox{for <span>$i,j=1,\ldots,n$</span>}$ \n H(x,y) := sum</em>{\ell=1}^m y<em>\ell H</em>\ell(x), where \n H<em>l(x)</em>{i,j} := partial^2 c<em>l(x) / partial x</em>i partial x<em>j \n for i,j=1,...,nis the Hessian of c</em>\ell(x)<span>$. Finally, for a given vector $v$</span>, we define the &lt;i&gt;residual-Hessians-vector product matrix&lt;/i&gt; <span>$P(x,v) := (H_1(x) v, \ldots, H_m(x) v).$</span> \n P(x,v) := (H<em>1(x) v, ..., H</em>m(x) v). \n The models <span>$t_k(s)$</span> provided are, -# the first-order Taylor approximation <span>$f(x_k) + g(x_k)^T s$</span>, where <span>$g(x) = J^T(x) W c(x)$</span>, -# a barely second-order approximation <span>$f(x_k) + g(x_k)^T s + \frac{1}{2} s^T W s$</span>, -# the Gauss-Newton approximation <span>$\frac{1}{2} \| c(x_k) + J(x_k) s\|^2_W$</span>, -# the Newton (second-order Taylor) approximation <span>$f(x_k) + g(x_k)^T s + \frac{1}{2} s^T [ J^T(x_k) W J(x_k) + H(x_k,W c(x_k))] s$</span>, and -# the tensor Gauss-Newton approximation <span>$\frac{1}{2} \| c(x_k) + J(x_k) s +  \frac{1}{2} s^T \cdot P(x_k,s) \|^2_W$</span>, where the <span>$i$</span>-th component of <span>$s^T \cdot P(x_k,s)$</span> is shorthand for the scalar <span>$s^T H_i(x_k) s$</span>, where <span>$W$</span> is the diagonal matrix of weights <span>$w_i$</span>, <span>$i = 1, \ldots m$</span>.</p><p>Access to a particular model requires that the user is either able to provide the derivatives needed (“&lt;i&gt;matrix available&lt;/i&gt;”) or that the products of these derivatives (and their transposes) with specified vectors are possible (“&lt;i&gt;matrix free&lt;/i&gt;”).</p><h2 id="Method"><a class="docs-heading-anchor" href="#Method">Method</a><a id="Method-1"></a><a class="docs-heading-anchor-permalink" href="#Method" title="Permalink"></a></h2><p>An adaptive regularization method is used. In this, an improvement to a current estimate of the required minimizer, <span>$x_k$</span> is sought by computing a step <span>$s_k$</span>. The step is chosen to approximately minimize a model <span>$t_k(s)$</span> of <span>$f_{\rho,r}(x_k+s)$</span> that includes a weighted regularization term <span>$(\sigma_k/p) \|s\|_{S_k}^p$</span> for some specified positive weight <span>$\sigma_k$</span>. The quality of the resulting step <span>$s_k$</span> is assessed by computing the &quot;ratio&quot; %<span>$(f_{\rho,p}(x_k) - f_{\rho,p}(x_k+s_k))/(t_k(0)-t_k(s_k))$</span>. <span>$(f(x_k) - f(x_k + s_k))/(t_k(0) - t_k(s_k))$</span>. The step is deemed to have succeeded if the ratio exceeds a given <span>$\eta_s &gt; 0$</span>, and in this case <span>$x_{k+1} = x_k + s_k$</span>. Otherwise <span>$x_{k+1} = x_k$</span>, and the weight is increased by powers of a given increase factor up to a given limit. If the ratio is larger than <span>$\eta_v \geq \eta_d$</span>, the weight will be decreased by powers of a given decrease factor again up to a given limit. The method will terminate as soon as <span>$f(x_k)$</span> or <span>$\|\nabla_x f(x_k)\|$</span> is smaller than a specified value.</p><p>A choice of linear, quadratic or quartic models <span>$t_k(s)$</span> is available (see the \ref section), and normally a two-norm regularization willbe used, but this may change if preconditioning is employed.</p><p>If linear or quadratic models are employed, an appropriate, approximate model minimizer is found using either a direct approach involving factorization of a shift of the model Hessian <span>$B_k$</span> or an iterative (conjugate-gradient/Lanczos) approach based on approximations to the required solution from a so-called Krlov subspace. The direct approach is based on the knowledge that the required solution satisfies the linear system of equations <span>$(B_k + \lambda_k I) s_k = - \nabla_x f(x_k)$</span> involving a scalar Lagrange multiplier <span>$\lambda_k$</span>. This multiplier is found by uni-variate root finding, using a safeguarded Newton-like process, by the GALAHAD packages RQS. The iterative approach uses the GALAHAD packag GLRT, and is best accelerated by preconditioning with good approximations to the Hessian of the model using GALAHAD&#39;s PSLS. The iterative approach has the advantage that only Hessian matrix-vector products are required, and thus the Hessian <span>$B_k$</span> is not required explicitly. However when factorizations of the Hessian are possible, the direct approach is often more efficient.</p><p>When a quartic model is used, the model is itself of least-squares form, and the package calls itself recursively to approximately minimize its model. The quartic model often gives a better approximation, but at the cost of more involved derivative requirements.</p><h2 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h2><p>The generic adaptive cubic regularization method is described in detail in</p><p>C. Cartis,N. I. M. Gould and Ph. L. Toint, “Adaptive cubic regularisation methods for unconstrained optimization. Part I: motivation, convergence and numerical results”, Mathematical Programming 127(2) (2011) 245-295,</p><p>and uses “tricks” as suggested in</p><p>N. I. M. Gould, M. Porcelli and Ph. L. Toint, “Updating the regularization parameter in the adaptive cubic regularization algorithm”. Computational Optimization and Applications 53(1) (2012) 1-22.</p><p>The specific methods employed here are discussed in</p><p>N. I. M. Gould, J. A. Scott and T. Rees, “Convergence and evaluation-complexity analysis of a regularized tensor-Newton method for solving nonlinear least-squares problems”. Computational Optimization and Applications 73(1) (2019) 1–35.</p><h2 id="Call-order"><a class="docs-heading-anchor" href="#Call-order">Call order</a><a id="Call-order-1"></a><a class="docs-heading-anchor-permalink" href="#Call-order" title="Permalink"></a></h2><p>To solve a given problem, functions from the nls package must be called in the following order:</p><ul><li>nls_initialize - provide default control parameters and set up initial data structures</li><li>nls_read_specfile (optional) - override control values by reading replacement values from a file</li><li>nls_import - set up problem data structures and fixed values</li><li>nls_reset_control (optional) - possibly change control parameters if a sequence of problems are being solved</li><li>solve the problem by calling one of</li><li>nls_solve_with_mat - solve using function calls to evaluate function, gradient and Hessian values</li><li>nls_solve_without_mat - solve using function calls to evaluate function and gradient values and Hessian-vector products</li><li>nls_solve_reverse_with_mat - solve returning to the calling program to obtain function, gradient and Hessian values, or</li><li>nls_solve_reverse_without_mat - solve returning to the calling prorgram to obtain function and gradient values and Hessian-vector products</li><li>nls_information (optional) - recover information about the solution and solution process</li><li>nls_terminate - deallocate data structures</li></ul><h2 id="Unsymmetric-matrix-storage-formats"><a class="docs-heading-anchor" href="#Unsymmetric-matrix-storage-formats">Unsymmetric matrix storage formats</a><a id="Unsymmetric-matrix-storage-formats-1"></a><a class="docs-heading-anchor-permalink" href="#Unsymmetric-matrix-storage-formats" title="Permalink"></a></h2><p>The unsymmetric <span>$m$</span> by <span>$n$</span> Jacobian matrix <span>$J \equiv \nabla_x c(x)$</span> and the residual-Hessians-vector product matrix <span>$P(x,v)$</span> may be presented and stored in a variety of convenient input formats. Let <span>$A$</span> be <span>$J$</span> or <span>$P$</span> as appropriate.</p><p>Both C-style (0 based)and fortran-style (1-based) indexing is allowed. Choose control.f_indexing as false for C style and true for fortran style; the discussion below presumes C style, but add 1 to indices for the corresponding fortran version.</p><p>Wrappers will automatically convert between 0-based (C) and 1-based (fortran) array indexing, so may be used transparently from C. This conversion involves both time and memory overheads that may be avoided by supplying data that is already stored using 1-based indexing.</p><h3 id="Dense-storage-format"><a class="docs-heading-anchor" href="#Dense-storage-format">Dense storage format</a><a id="Dense-storage-format-1"></a><a class="docs-heading-anchor-permalink" href="#Dense-storage-format" title="Permalink"></a></h3><p>The matrix <span>$A$</span> is stored as a compactdense matrix by rows, that is, the values of the entries of each row in turn are stored in order within an appropriate real one-dimensional array. In this case, component <span>$n \ast i + j$</span>of the storage array A<em>val will hold the value A</em>{ij}$ for <span>$0 \leq i \leq m-1$</span>, <span>$0 \leq j \leq n-1$</span>.</p><h3 id="unsymmetric_matrix*dense*cols-Dense-by-columns-storage-format"><a class="docs-heading-anchor" href="#unsymmetric_matrix*dense*cols-Dense-by-columns-storage-format">unsymmetric_matrix<em>dense</em>cols Dense by columns storage format</a><a id="unsymmetric_matrix*dense*cols-Dense-by-columns-storage-format-1"></a><a class="docs-heading-anchor-permalink" href="#unsymmetric_matrix*dense*cols-Dense-by-columns-storage-format" title="Permalink"></a></h3><p>The matrix <span>$A$</span> is stored as a compactdense matrix by columns, that is, the values of the entries of each column in turn are stored in order within an appropriate real one-dimensional array. In this case, component <span>$m \ast j + i$</span>of the storage array A<em>val will hold the value A</em>{ij}$ for <span>$0 \leq i \leq m-1$</span>, <span>$0 \leq j \leq n-1$</span>.</p><h3 id="Sparse-co-ordinate-storage-format"><a class="docs-heading-anchor" href="#Sparse-co-ordinate-storage-format">Sparse co-ordinate storage format</a><a id="Sparse-co-ordinate-storage-format-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-co-ordinate-storage-format" title="Permalink"></a></h3><p>Only the nonzero entries of the matrices are stored. For the <span>$l$</span>-th entry, <span>$0 \leq l \leq ne-1$</span>, of <span>$A$</span>, its row index i, column index j and value <span>$A_{ij}$</span>, <span>$0 \leq i \leq m-1$</span>,<span>$0 \leq j \leq n-1$</span>,are stored as the <span>$l$</span>-th components of the integer arrays A<em>row and A</em>col and real array A<em>val, respectively, while the number of nonzeros is recorded as A</em>ne = <span>$ne$</span>.</p><h3 id="Sparse-row-wise-storage-format"><a class="docs-heading-anchor" href="#Sparse-row-wise-storage-format">Sparse row-wise storage format</a><a id="Sparse-row-wise-storage-format-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-row-wise-storage-format" title="Permalink"></a></h3><p>Again only the nonzero entries are stored, but this time they are ordered so that those in row i appear directly before those in row i+1. For the i-th row of <span>$A$</span> the i-th component of the integer array A<em>ptr holds the position of the first entry in this row, while A</em>ptr(m) holds the total number of entries plus one. The column indices j, <span>$0 \leq j \leq n-1$</span>, and values <span>$A_{ij}$</span> of thenonzero entries in the i-th row are stored in components l = A<em>ptr(i), <span>$\ldots$</span>, A</em>ptr(i+1)-1,<span>$0 \leq i \leq m-1$</span>, of the integer array A<em>col, and real array A</em>val, respectively. For sparse matrices, this scheme almost always requires less storage than its predecessor.</p><h3 id="unsymmetric_matrix*column*wise-Sparse-column-wise-storage-format"><a class="docs-heading-anchor" href="#unsymmetric_matrix*column*wise-Sparse-column-wise-storage-format">unsymmetric_matrix<em>column</em>wise Sparse column-wise storage format</a><a id="unsymmetric_matrix*column*wise-Sparse-column-wise-storage-format-1"></a><a class="docs-heading-anchor-permalink" href="#unsymmetric_matrix*column*wise-Sparse-column-wise-storage-format" title="Permalink"></a></h3><p>Once again only the nonzero entries are stored, but this time they are ordered so that those in column j appear directly before those in column j+1. For the j-th column of <span>$A$</span> the j-th component of the integer array A<em>ptr holds the position of the first entry in this column, while A</em>ptr(n) holds the total number of entries plus one. The row indices i, <span>$0 \leq i \leq m-1$</span>, and values <span>$A_{ij}$</span> of thenonzero entries in the j-th columnsare stored in components l = A<em>ptr(j), <span>$\ldots$</span>, A</em>ptr(j+1)-1, <span>$0 \leq j \leq n-1$</span>, of the integer array A<em>row, and real array A</em>val, respectively. As before, for sparse matrices, this scheme almost always requires less storage than the co-ordinate format.</p><h2 id="Symmetric-matrix-storage-formats"><a class="docs-heading-anchor" href="#Symmetric-matrix-storage-formats">Symmetric matrix storage formats</a><a id="Symmetric-matrix-storage-formats-1"></a><a class="docs-heading-anchor-permalink" href="#Symmetric-matrix-storage-formats" title="Permalink"></a></h2><p>Likewise, the symmetric <span>$n$</span> by <span>$n$</span> weighted-residual Hessian matrix <span>$H = H(x,y)$</span> may be presented and stored in a variety of formats. But crucially symmetry is exploited by only storing values from the lower triangular part (i.e, those entries that lie on or below the leading diagonal).</p><h3 id="Dense-storage-format-2"><a class="docs-heading-anchor" href="#Dense-storage-format-2">Dense storage format</a><a class="docs-heading-anchor-permalink" href="#Dense-storage-format-2" title="Permalink"></a></h3><p>The matrix <span>$H$</span> is stored as a compactdense matrix by rows, that is, the values of the entries of each row in turn are stored in order within an appropriate real one-dimensional array. Since <span>$H$</span> is symmetric, only the lower triangular part (that is the part <span>$h_{ij}$</span> for <span>$0 \leq j \leq i \leq n-1$</span>) need be held. In this case the lower triangle should be stored by rows, that is component <span>$i \ast i / 2 + j$</span>of the storage array H<em>val will hold the value h</em>{ij}$ (and, by symmetry, <span>$h_{ji}$</span>) for <span>$0 \leq j \leq i \leq n-1$</span>.</p><h3 id="Sparse-co-ordinate-storage-format-2"><a class="docs-heading-anchor" href="#Sparse-co-ordinate-storage-format-2">Sparse co-ordinate storage format</a><a class="docs-heading-anchor-permalink" href="#Sparse-co-ordinate-storage-format-2" title="Permalink"></a></h3><p>Only the nonzero entries of the matrices are stored. For the <span>$l$</span>-th entry, <span>$0 \leq l \leq ne-1$</span>, of <span>$H$</span>, its row index i, column index j and value <span>$h_{ij}$</span>, <span>$0 \leq j \leq i \leq n-1$</span>,are stored as the <span>$l$</span>-th components of the integer arrays H<em>row and H</em>col and real array H<em>val, respectively, while the number of nonzeros is recorded as H</em>ne = <span>$ne$</span>. Note that only the entries in the lower triangle should be stored.</p><h3 id="Sparse-row-wise-storage-format-2"><a class="docs-heading-anchor" href="#Sparse-row-wise-storage-format-2">Sparse row-wise storage format</a><a class="docs-heading-anchor-permalink" href="#Sparse-row-wise-storage-format-2" title="Permalink"></a></h3><p>Again only the nonzero entries are stored, but this time they are ordered so that those in row i appear directly before those in row i+1. For the i-th row of <span>$H$</span> the i-th component of the integer array H<em>ptr holds the position of the first entry in this row, while H</em>ptr(n) holds the total number of entries plus one. The column indices j, <span>$0 \leq j \leq i$</span>, and values <span>$h_{ij}$</span> of theentries in the i-th row are stored in components l = H<em>ptr(i), <span>$\ldots$</span>, H</em>ptr(i+1)-1 of the integer array H<em>col, and real array H</em>val, respectively. Note that as before only the entries in the lower triangle should be stored. For sparse matrices, this scheme almost always requires less storage than its predecessor.</p><h3 id="symmetric_matrix_diagonal-Diagonal-storage-format"><a class="docs-heading-anchor" href="#symmetric_matrix_diagonal-Diagonal-storage-format">symmetric_matrix_diagonal Diagonal storage format</a><a id="symmetric_matrix_diagonal-Diagonal-storage-format-1"></a><a class="docs-heading-anchor-permalink" href="#symmetric_matrix_diagonal-Diagonal-storage-format" title="Permalink"></a></h3><p>If <span>$H$</span> is diagonal (i.e., <span>$H_{ij} = 0$</span> for all <span>$0 \leq i \neq j \leq n-1$</span>) only the diagonals entries <span>$H_{ii}$</span>, <span>$0 \leq i \leq n-1$</span> need be stored, and the first n components of the array H_val may be used for the purpose.</p><h3 id="symmetric_matrix*scaled*identity-Multiples-of-the-identity-storage-format"><a class="docs-heading-anchor" href="#symmetric_matrix*scaled*identity-Multiples-of-the-identity-storage-format">symmetric_matrix<em>scaled</em>identity Multiples of the identity storage format</a><a id="symmetric_matrix*scaled*identity-Multiples-of-the-identity-storage-format-1"></a><a class="docs-heading-anchor-permalink" href="#symmetric_matrix*scaled*identity-Multiples-of-the-identity-storage-format" title="Permalink"></a></h3><p>If <span>$H$</span> is a multiple of the identity matrix, (i.e., <span>$H = \alpha I$</span> where <span>$I$</span> is the n by n identity matrix and <span>$\alpha$</span> is a scalar), it suffices to store <span>$\alpha$</span> as the first component of H_val.</p><h3 id="symmetric_matrix_identity-The-identity-matrix-format"><a class="docs-heading-anchor" href="#symmetric_matrix_identity-The-identity-matrix-format">symmetric_matrix_identity The identity matrix format</a><a id="symmetric_matrix_identity-The-identity-matrix-format-1"></a><a class="docs-heading-anchor-permalink" href="#symmetric_matrix_identity-The-identity-matrix-format" title="Permalink"></a></h3><p>If <span>$H$</span> is the identity matrix, no values need be stored.</p><h3 id="symmetric_matrix_zero-The-zero-matrix-format"><a class="docs-heading-anchor" href="#symmetric_matrix_zero-The-zero-matrix-format">symmetric_matrix_zero The zero matrix format</a><a id="symmetric_matrix_zero-The-zero-matrix-format-1"></a><a class="docs-heading-anchor-permalink" href="#symmetric_matrix_zero-The-zero-matrix-format" title="Permalink"></a></h3><p>The same is true if <span>$H$</span> is the zero matrix.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../lstr/">« lstr</a><a class="docs-footer-nextpage" href="../presolve/">presolve »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 7 December 2022 22:13">Wednesday 7 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
